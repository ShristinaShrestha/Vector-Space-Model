{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XAND4BpPdaD",
        "outputId": "e81792cb-34db-4854-f3c5-885c3d578976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text files from the folder\n",
        "def load_text_files(folder_path):\n",
        "    data = []\n",
        "    doc_id_to_filename = {}\n",
        "    for i, filename in enumerate(os.listdir(folder_path)):\n",
        "        if filename.endswith('.txt'):  # Ensure it's a text file\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data.append(file.read())\n",
        "                doc_id_to_filename[i] = filename\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "H8eQo42PQOIo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path\n",
        "folder_path = '/content/drive/MyDrive/document'"
      ],
      "metadata": {
        "id": "COhTy5CuQfUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "docs, doc_id_to_filename = load_text_files(folder_path)"
      ],
      "metadata": {
        "id": "MooFinxoQlIx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries with logical operators\n",
        "queries = [\n",
        "    \"Healthy eating AND disease prevention\",\n",
        "    \"Vitamins in fruits AND immunity\",\n",
        "    \"Whole grains: cholesterol OR sugar\",\n",
        "    \"Plant OR animal proteins\",\n",
        "    \"Processed food AND heart/weight\"\n",
        "]\n",
        "\n",
        "# Preprocess documents and queries: lowercase and tokenize\n",
        "def tokenize(text):\n",
        "  return text.lower().split()\n",
        "tokenized_docs = [tokenize(doc) for doc in docs]\n",
        "tokenized_queries = [tokenize(query) for query in queries]"
      ],
      "metadata": {
        "id": "AUrtFljtQoyi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Performs text cleaning: removing special characters, digits, tokenization, stopword removal, and lemmatization.\"\"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation using regular expressions (keeps only alphanumeric and spaces)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Remove all digits\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Removes digits globally\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "OJ0De3nUVtiP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary (unique words across all documents and queries)\n",
        "vocab = set([word for doc in tokenized_docs for word in doc if word.isalpha()])\n",
        "vocab = sorted(vocab)  # Optional sorting for consistency\n",
        "print(\"Vocabulary:\", vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKJsNVfIeBoi",
        "outputId": "d0ca7e59-961e-4be7-a048-45fa35c2e5f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['a', 'achieving', 'acids', 'addition', 'additional', 'additives', 'after', 'aiding', 'aids', 'all', 'almond', 'along', 'also', 'alternatives', 'amino', 'an', 'and', 'another', 'apples', 'are', 'as', 'aspect', 'b', 'balance', 'balanced', 'be', 'blood', 'body', 'bone', 'bowel', 'broccoli', 'brown', 'build', 'by', 'c', 'calcium', 'calorie', 'can', 'carbohydrates', 'certain', 'cholesterol', 'choosing', 'chronic', 'cognitive', 'colon', 'colorful', 'combination', 'complete', 'components', 'conditions', 'consumed', 'consuming', 'contain', 'contribute', 'critical', 'crucial', 'cruciferous', 'daily', 'dairy', 'detoxification', 'diet', 'digestion', 'disease', 'diseases', 'drinking', 'drinks', 'each', 'eating', 'eggs', 'energy', 'enough', 'ensure', 'enzymes', 'especially', 'essential', 'excellent', 'fats', 'feeling', 'fiber', 'foods', 'for', 'fortified', 'foundation', 'from', 'fruits', 'full', 'fullness', 'function', 'gastrointestinal', 'glasses', 'good', 'grain', 'grains', 'greens', 'growth', 'habits', 'harmful', 'health', 'healthy', 'heart', 'help', 'helps', 'high', 'hydration', 'if', 'immune', 'impact', 'improve', 'in', 'including', 'increased', 'increasing', 'intake', 'is', 'it', 'keeping', 'key', 'known', 'leafy', 'lean', 'levels', 'like', 'limiting', 'long', 'loss', 'lower', 'macronutrient', 'magnesium', 'maintain', 'maintaining', 'management', 'meals', 'meaning', 'means', 'mental', 'metabolism', 'minerals', 'mood', 'more', 'most', 'muscle', 'natural', 'necessary', 'needed', 'negatively', 'nutrient', 'nutrients', 'nutrition', 'of', 'offer', 'often', 'optimally', 'options', 'or', 'over', 'overall', 'overlooked', 'oxidative', 'part', 'particularly', 'parts', 'physical', 'plays', 'potassium', 'preservatives', 'prevent', 'preventing', 'processed', 'produce', 'promote', 'promoting', 'protein', 'proteins', 'provide', 'provides', 'quinoa', 'receives', 'reduce', 'reducing', 'refined', 'regular', 'regulate', 'regulates', 'repair', 'replacing', 'retain', 'rich', 'risk', 'role', 'satiety', 'significantly', 'skin', 'sources', 'spectrum', 'spinach', 'steady', 'strong', 'such', 'sugary', 'support', 'supporting', 'supports', 'that', 'the', 'them', 'these', 'they', 'throughout', 'to', 'transporting', 'type', 'typically', 'unhealthy', 'unlike', 'unnecessary', 'unprocessed', 'variety', 'vegetables', 'vegetarians', 'vibrant', 'vital', 'vitamin', 'vitamins', 'water', 'weight', 'which', 'whole', 'with', 'yet', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate term frequency (TF)\n",
        "def term_frequency(term, document):\n",
        "  return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "QA88dqSEV5WT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate inverse document frequency (IDF)\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "  num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "  return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "soN2RWi6YRct"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF for a document\n",
        "def compute_tfidf(document, all_documents, vocab):\n",
        "  tfidf_vector = []\n",
        "  for term in vocab:\n",
        "    tf = term_frequency(term, document)\n",
        "    idf = inverse_document_frequency(term, all_documents)\n",
        "    tfidf_vector.append(tf * idf)\n",
        "  return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "eg3zFSWnV-md"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  norm_vec1 = np.linalg.norm(vec1)\n",
        "  norm_vec2 = np.linalg.norm(vec2)\n",
        "  return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "metadata": {
        "id": "YR0hi1D0YcVR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF vectors for documents and queries\n",
        "doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]"
      ],
      "metadata": {
        "id": "VCgwjSnhYuHw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for the output file\n",
        "output_file_path = \"/content/drive/MyDrive/result_Shristina.txt\"\n",
        "\n",
        "# Opening the file in write mode\n",
        "with open(output_file_path, 'w') as f:\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = []\n",
        "    for query_vector in query_tfidf_vectors:\n",
        "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
        "        cosine_similarities.append(similarities)\n",
        "\n",
        "    # Displaying the results in ascending order of cosine similarity\n",
        "    for i, query in enumerate(queries):\n",
        "        f.write(f\"\\nCosine similarities for query '{query}':\\n\")\n",
        "\n",
        "        # Zipping document indices and their corresponding similarities\n",
        "        doc_sim_pairs = list(enumerate(cosine_similarities[i]))\n",
        "\n",
        "        # Sorting the pairs based on similarity in ascending order\n",
        "        doc_sim_pairs_sorted = sorted(doc_sim_pairs, key=lambda x: x[1])\n",
        "\n",
        "        # Writing the sorted document similarities to the file\n",
        "        for doc_idx, similarity in doc_sim_pairs_sorted:\n",
        "            f.write(f\"Document {doc_idx + 1}: {similarity:.4f}\\n\")\n",
        "\n",
        "# Confirming that the output has been saved\n",
        "print(f\"Output has been saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMpWMmOlZBJd",
        "outputId": "e4c07bd2-ac0d-4c90-f6b9-6908b9ccb836"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output has been saved to /content/drive/MyDrive/result_Shristina.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnpApoohtoyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}